{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9da9b857-4dd5-4486-b232-a07de14469b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load('quandles-alpha.sage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f612ffe-608f-4374-836b-d3878996ad41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arcs: [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Crossings summary:\n",
      " 1: pd=(4, 5, 8, 1), arcs=[1, 4, 5, 8], sign=+, det=-2.22045e-16\n",
      " 2: pd=(1, 2, 5, 6), arcs=[1, 2, 5, 6], sign=-, det=-0.00387999\n",
      " 3: pd=(6, 7, 2, 3), arcs=[2, 3, 6, 7], sign=+, det=0.00387999\n",
      " 4: pd=(3, 4, 7, 8), arcs=[3, 4, 7, 8], sign=-, det=-0.00387999\n",
      "Arc -> crossings: {1: [1, 2], 2: [2, 3], 3: [3, 4], 4: [1, 4], 5: [1, 2], 6: [2, 3], 7: [3, 4], 8: [1, 4]}\n"
     ]
    }
   ],
   "source": [
    "# SageMath-ready PD analyzer\n",
    "# Paste this into a Sage cell (sage kernel) and run.\n",
    "\n",
    "from sage.all import *\n",
    "import math\n",
    "\n",
    "def unique_sorted(iterable):\n",
    "    seen = []\n",
    "    for x in iterable:\n",
    "        if x not in seen:\n",
    "            seen.append(x)\n",
    "    return sorted(seen)\n",
    "\n",
    "def place_arc_points_on_circle_sage(arcs, radius=1.0):\n",
    "    \"\"\"\n",
    "    Place each arc label deterministically on a circle and return a dict label -> (x,y)\n",
    "    Uses a small deterministic phase jitter per label to avoid perfect symmetry.\n",
    "    \"\"\"\n",
    "    m = len(arcs)\n",
    "    pts = {}\n",
    "    for i, a in enumerate(arcs):\n",
    "        theta = 2 * math.pi * i / max(1, m)\n",
    "        # deterministic small jitter based on label\n",
    "        j = ((int(a) * 97) % 1000 - 500) / 100000.0\n",
    "        th = theta + j\n",
    "        x = radius * math.cos(th)\n",
    "        y = radius * math.sin(th)\n",
    "        pts[a] = (float(x), float(y))\n",
    "    return pts\n",
    "\n",
    "def vector_sub(p, q):\n",
    "    return (p[0]-q[0], p[1]-q[1])\n",
    "\n",
    "def det2(u, v):\n",
    "    return u[0]*v[1] - u[1]*v[0]\n",
    "\n",
    "def normalize(v):\n",
    "    norm = math.hypot(v[0], v[1])\n",
    "    if norm == 0:\n",
    "        return (0.0, 0.0)\n",
    "    return (v[0]/norm, v[1]/norm)\n",
    "\n",
    "def safe_tangent(p_from, p_to):\n",
    "    v = vector_sub(p_to, p_from)\n",
    "    if abs(v[0]) < 1e-12 and abs(v[1]) < 1e-12:\n",
    "        # tiny deterministic fallback\n",
    "        v = (1e-6, 0.0)\n",
    "    return normalize(v)\n",
    "\n",
    "def analyze_pd_sage(pd, ensure_non_degenerate=True):\n",
    "    \"\"\"\n",
    "    Analyze PD in Sage. pd is a list of 4-tuples (a,b,c,d) where (a->b) is under strand,\n",
    "    (c->d) is over strand. Returns a dictionary with arcs, pts, crossings and arc->crossing map.\n",
    "    \"\"\"\n",
    "    # collect arcs\n",
    "    arcs = []\n",
    "    for tup in pd:\n",
    "        arcs.extend([tup[0], tup[1], tup[2], tup[3]])\n",
    "    arcs = unique_sorted(arcs)\n",
    "    # place points\n",
    "    pts = place_arc_points_on_circle_sage(arcs, radius=1.0)\n",
    "\n",
    "    crossings = {}\n",
    "    arc_to_crossings = {a: [] for a in arcs}\n",
    "\n",
    "    for i, tup in enumerate(pd, start=1):\n",
    "        a,b,c,d = tup\n",
    "        p_a = pts[a]; p_b = pts[b]; p_c = pts[c]; p_d = pts[d]\n",
    "\n",
    "        # compute tangents (under and over)\n",
    "        # use direction from a->b for under, c->d for over (consistent orientation)\n",
    "        t_under = safe_tangent(p_b, p_a)\n",
    "        t_over  = safe_tangent(p_d, p_c)\n",
    "        det = det2(t_under, t_over)\n",
    "        # evaluate sign with tolerance\n",
    "        tol = 1e-9\n",
    "        if det > tol:\n",
    "            sign = 1\n",
    "        elif det < -tol:\n",
    "            sign = -1\n",
    "        else:\n",
    "            sign = 0\n",
    "\n",
    "        # if degenerate, attempt small deterministic perturbations\n",
    "        if sign == 0 and ensure_non_degenerate:\n",
    "            success = False\n",
    "            for trial in range(6):\n",
    "                # copy pts and perturb only involved labels slightly based on trial and label\n",
    "                pts_trial = dict(pts)\n",
    "                for label in (a,b,c,d):\n",
    "                    x,y = pts_trial[label]\n",
    "                    eps = (0.00025 + 0.00005*trial) * (((int(label)*31) % 7) - 3)\n",
    "                    th = math.atan2(y,x) + eps\n",
    "                    r = math.hypot(x,y)\n",
    "                    pts_trial[label] = (r * math.cos(th), r * math.sin(th))\n",
    "                p_a = pts_trial[a]; p_b = pts_trial[b]; p_c = pts_trial[c]; p_d = pts_trial[d]\n",
    "                t_under = safe_tangent(p_b, p_a)\n",
    "                t_over  = safe_tangent(p_d, p_c)\n",
    "                det_trial = det2(t_under, t_over)\n",
    "                if abs(det_trial) > tol:\n",
    "                    det = det_trial\n",
    "                    sign = 1 if det > 0 else -1\n",
    "                    pts = pts_trial\n",
    "                    success = True\n",
    "                    break\n",
    "            if not success:\n",
    "                # fallback: determine sign from a deterministic rule based on tuple ordering:\n",
    "                # This fallback is only used when geometry fails to disambiguate.\n",
    "                sign = 1\n",
    "\n",
    "        crossings[i] = {\n",
    "            'pd_tuple': tup,\n",
    "            'under_arc': (a,b),\n",
    "            'over_arc': (c,d),\n",
    "            'arcs_involved': sorted({a,b,c,d}),\n",
    "            't_under': t_under,\n",
    "            't_over': t_over,\n",
    "            'det': float(det),\n",
    "            'sign': int(sign),\n",
    "            'sign_label': '+' if sign == 1 else ('-' if sign == -1 else '?')\n",
    "        }\n",
    "        for label in (a,b,c,d):\n",
    "            arc_to_crossings.setdefault(label, []).append(i)\n",
    "\n",
    "    return {\n",
    "        'arcs': arcs,\n",
    "        'pts': pts,\n",
    "        'crossings': crossings,\n",
    "        'arc_to_crossings': arc_to_crossings\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------- Example run on figure-eight PD ----------------\n",
    "pd_fig8 = [(4,5,8,1), (1,2,5,6), (6,7,2,3), (3,4,7,8)]\n",
    "kd = analyze_pd_sage(pd_fig8)\n",
    "\n",
    "print(\"Arcs:\", kd['arcs'])\n",
    "print(\"Crossings summary:\")\n",
    "for ci, info in kd['crossings'].items():\n",
    "    print(\" %d: pd=%s, arcs=%s, sign=%s, det=%.6g\" % (ci, str(info['pd_tuple']), str(info['arcs_involved']), info['sign_label'], info['det']))\n",
    "print(\"Arc -> crossings:\", kd['arc_to_crossings'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b829756-9a37-4fa3-9e3d-5a7b05837edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original arcs (pre-merge): [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "\n",
      "Merged groups (gID -> original arcs):\n",
      " g1: [1, 7, 8]\n",
      " g2: [2, 3]\n",
      " g3: [4]\n",
      " g4: [5, 6]\n",
      "\n",
      "Crossings (merged tuples + signs):\n",
      " 1: pd_merged=(3, 4, 1, 1), sign=+, det(original)=0\n",
      " 2: pd_merged=(1, 2, 4, 4), sign=-, det(original)=-0.00148\n",
      " 3: pd_merged=(4, 1, 2, 2), sign=+, det(original)=0.00148\n",
      " 4: pd_merged=(2, 3, 1, 1), sign=-, det(original)=-0.00148\n",
      "\n",
      "Arc -> crossings (merged labels):\n",
      " arc 1: [1, 1, 2, 3, 4, 4]\n",
      " arc 2: [2, 3, 3, 4]\n",
      " arc 3: [1, 4]\n",
      " arc 4: [1, 2, 2, 3]\n",
      "\n",
      "Total merged arcs: 4\n"
     ]
    }
   ],
   "source": [
    "# Sage cell: compute signs first, then merge arcs for naming\n",
    "from sage.all import *\n",
    "import math\n",
    "\n",
    "# ---------------- geometry helpers ----------------\n",
    "def vector_sub(p, q): return (p[0]-q[0], p[1]-q[1])\n",
    "def det2(u, v): return u[0]*v[1] - u[1]*v[0]\n",
    "def normalize(v):\n",
    "    n = math.hypot(v[0], v[1])\n",
    "    return (v[0]/n, v[1]/n) if n else (0.0, 0.0)\n",
    "def safe_tangent(p_from, p_to):\n",
    "    v = vector_sub(p_to, p_from)\n",
    "    if abs(v[0]) < 1e-12 and abs(v[1]) < 1e-12:\n",
    "        v = (1e-6, 0.0)\n",
    "    return normalize(v)\n",
    "\n",
    "# ---------------- place arc points (original, distinct arcs) ----------------\n",
    "def place_arc_points(arcs, radius=1.0):\n",
    "    \"\"\"Place each original arc label at a distinct point on a circle (deterministic jitter).\"\"\"\n",
    "    m = max(1, len(arcs))\n",
    "    pts = {}\n",
    "    for i, a in enumerate(sorted(arcs)):\n",
    "        th = 2 * math.pi * i / m\n",
    "        j = ((int(a) * 37) % 1000 - 500) / 100000.0\n",
    "        th += j\n",
    "        pts[a] = (radius * math.cos(th), radius * math.sin(th))\n",
    "    return pts\n",
    "\n",
    "# ---------------- compute signs from original PD ----------------\n",
    "def compute_signs_from_pd(pd):\n",
    "    \"\"\"\n",
    "    Input: pd list of 4-tuples (a,b,c,d) using standard PD convention:\n",
    "           (a->b) is under strand, (c->d) is over strand.\n",
    "    Returns dict crossing_index -> info with det, sign, sign_label and original pd tuple.\n",
    "    \"\"\"\n",
    "    # collect original arcs\n",
    "    arc_set = sorted({x for tup in pd for x in tup})\n",
    "    pts = place_arc_points(arc_set)\n",
    "    crossings = {}\n",
    "    tol = 1e-9\n",
    "    for i, (a,b,c,d) in enumerate(pd, start=1):\n",
    "        p_a, p_b, p_c, p_d = pts[a], pts[b], pts[c], pts[d]\n",
    "        # choose tangent directions consistent with traversal (a->b under, c->d over)\n",
    "        t_under = safe_tangent(p_b, p_a)\n",
    "        t_over  = safe_tangent(p_d, p_c)\n",
    "        det = det2(t_under, t_over)\n",
    "        if det > tol:\n",
    "            sign = 1\n",
    "        elif det < -tol:\n",
    "            sign = -1\n",
    "        else:\n",
    "            # attempt small deterministic perturbation of involved points if degenerate\n",
    "            sign = 0\n",
    "            for trial in range(6):\n",
    "                pts_trial = dict(pts)\n",
    "                for label in (a,b,c,d):\n",
    "                    x,y = pts_trial[label]\n",
    "                    eps = (0.00025 + 0.00005*trial) * (((int(label)*31) % 7) - 3)\n",
    "                    th = math.atan2(y,x) + eps\n",
    "                    r = math.hypot(x,y)\n",
    "                    pts_trial[label] = (r * math.cos(th), r * math.sin(th))\n",
    "                t_under = safe_tangent(pts_trial[b], pts_trial[a])\n",
    "                t_over  = safe_tangent(pts_trial[d], pts_trial[c])\n",
    "                det_trial = det2(t_under, t_over)\n",
    "                if abs(det_trial) > tol:\n",
    "                    det = det_trial\n",
    "                    sign = 1 if det > 0 else -1\n",
    "                    pts = pts_trial\n",
    "                    break\n",
    "            # if still zero, default to +1 to avoid None\n",
    "            if sign == 0:\n",
    "                sign = 1\n",
    "        crossings[i] = {\n",
    "            'pd_tuple_original': (a,b,c,d),\n",
    "            'under_arc_original': (a,b),\n",
    "            'over_arc_original': (c,d),\n",
    "            'det': float(det),\n",
    "            'sign': int(sign),\n",
    "            'sign_label': '+' if sign==1 else '-'\n",
    "        }\n",
    "    return crossings, arc_set\n",
    "\n",
    "# ---------------- merge arcs by over-arc connectivity ----------------\n",
    "def merge_arcs_from_pd(pd):\n",
    "    \"\"\"\n",
    "    Merge arc labels according to over-arc connectivity:\n",
    "      • for each over arc (c,d), merge c and d\n",
    "      • merging is transitive\n",
    "      • under arcs (a,b) remain distinct\n",
    "    \"\"\"\n",
    "    arc_set = set()\n",
    "    for a,b,c,d in pd:\n",
    "        arc_set.update([a,b,c,d])\n",
    "    parent = {x:x for x in arc_set}\n",
    "    def find(x):\n",
    "        while parent[x] != x:\n",
    "            parent[x] = parent[parent[x]]\n",
    "            x = parent[x]\n",
    "        return x\n",
    "    def union(x,y):\n",
    "        rx, ry = find(x), find(y)\n",
    "        if rx==ry: return\n",
    "        if rx<ry: parent[ry]=rx\n",
    "        else: parent[rx]=ry\n",
    "    # merge using only over arcs\n",
    "    for a,b,c,d in pd:\n",
    "        union(c,d)\n",
    "\n",
    "    groups={}\n",
    "    for x in sorted(arc_set):\n",
    "        r=find(x)\n",
    "        groups.setdefault(r,[]).append(x)\n",
    "\n",
    "    reps=sorted(groups.keys())\n",
    "    rep_to_gid={rep:i+1 for i,rep in enumerate(reps)}\n",
    "    gid_to_members={rep_to_gid[r]:groups[r] for r in reps}\n",
    "    label_to_group={label:rep_to_gid[find(label)] for label in arc_set}\n",
    "\n",
    "    pd_merged=[(label_to_group[a],label_to_group[b],\n",
    "                 label_to_group[c],label_to_group[d]) for a,b,c,d in pd]\n",
    "    return {\n",
    "        'groups':gid_to_members,\n",
    "        'label_to_group':label_to_group,\n",
    "        'pd_merged':pd_merged,\n",
    "        'reduced_arcs':sorted(gid_to_members.keys())\n",
    "    }\n",
    "# ---------------- main analyzer: signs first, then merge ----------------\n",
    "def analyze_pd_sage(pd):\n",
    "    # 1) compute crossing signs from original PD geometry\n",
    "    crossings_raw, original_arcs = compute_signs_from_pd(pd)\n",
    "\n",
    "    # 2) merge arcs (over-arc connectivity)\n",
    "    merged = merge_arcs_from_pd(pd)\n",
    "    pdm = merged['pd_merged']\n",
    "    merged_arcs = merged['reduced_arcs']\n",
    "\n",
    "    # 3) build arc->crossings mapping using merged labels\n",
    "    arc_to_crossings = {g: [] for g in merged_arcs}\n",
    "    crossings_final = {}\n",
    "    # For each crossing index preserve sign computed earlier, but attach merged pd tuple\n",
    "    for i, orig_info in crossings_raw.items():\n",
    "        # original PD tuple:\n",
    "        a,b,c,d = orig_info['pd_tuple_original']\n",
    "        # merged tuple:\n",
    "        a_m, b_m, c_m, d_m = merged['label_to_group'][a], merged['label_to_group'][b], merged['label_to_group'][c], merged['label_to_group'][d]\n",
    "        # record final crossing info including preserved sign\n",
    "        crossings_final[i] = {\n",
    "            'pd_tuple_original': (a,b,c,d),\n",
    "            'pd_tuple_merged': (a_m, b_m, c_m, d_m),\n",
    "            'under_arc_merged': (a_m, b_m),\n",
    "            'over_arc_merged': (c_m, d_m),\n",
    "            'det_original': crossings_raw[i]['det'],\n",
    "            'sign': crossings_raw[i]['sign'],\n",
    "            'sign_label': crossings_raw[i]['sign_label']\n",
    "        }\n",
    "        # add to arc->crossing lists (use merged labels)\n",
    "        for lab in (a_m, b_m, c_m, d_m):\n",
    "            arc_to_crossings.setdefault(lab, []).append(i)\n",
    "\n",
    "    # 4) build knot_info-like object (dict)\n",
    "    knot_info = {\n",
    "        'original_arcs': original_arcs,\n",
    "        'merged': merged,\n",
    "        'arcs': merged_arcs,\n",
    "        'crossings': crossings_final,\n",
    "        'arc_to_crossings': arc_to_crossings\n",
    "    }\n",
    "    return knot_info\n",
    "\n",
    "# ---------------- Example: figure-eight PD ----------------\n",
    "pd_fig8 = [(4,5,8,1), (1,2,5,6), (6,7,2,3), (3,4,7,8)]\n",
    "kinfo = analyze_pd_sage(pd_fig8)\n",
    "\n",
    "print(\"Original arcs (pre-merge):\", kinfo['original_arcs'])\n",
    "print(\"\\nMerged groups (gID -> original arcs):\")\n",
    "for gid, members in kinfo['merged']['groups'].items():\n",
    "    print(\" g{}: {}\".format(gid, members))\n",
    "print(\"\\nCrossings (merged tuples + signs):\")\n",
    "for ci,info in sorted(kinfo['crossings'].items()):\n",
    "    print(\" {}: pd_merged={}, sign={}, det(original)={:.6g}\".format(ci, info['pd_tuple_merged'], info['sign_label'], info['det_original']))\n",
    "print(\"\\nArc -> crossings (merged labels):\")\n",
    "for a,xs in sorted(kinfo['arc_to_crossings'].items()):\n",
    "    print(\" arc {}: {}\".format(a, xs))\n",
    "print(\"\\nTotal merged arcs:\", len(kinfo['arcs']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc007ac7-6ff8-42b7-adda-f9ab8d225153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 2, 0, 3), (3, 0, 4, 1), (1, 4, 2, 5)]\n"
     ]
    }
   ],
   "source": [
    "import snappy\n",
    "M = snappy.Link('3_1')\n",
    "pd = M.PD_code()       # ← works for both knots and links\n",
    "print(pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23c54fc7-b1e3-492e-adce-8d7a5e1859f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original arcs (pre-merge): [0, 1, 2, 3, 4, 5]\n",
      "\n",
      "Merged groups (gID -> original arcs):\n",
      " g1: [0, 3]\n",
      " g2: [1, 4]\n",
      " g3: [2, 5]\n",
      "\n",
      "Crossings (merged tuples + signs):\n",
      " 1: pd_merged=(3, 3, 1, 1), sign=+, det(original)=0.865655\n",
      " 2: pd_merged=(1, 1, 2, 2), sign=+, det(original)=0.86621\n",
      " 3: pd_merged=(2, 2, 3, 3), sign=+, det(original)=0.86621\n",
      "\n",
      "Arc -> crossings (merged labels):\n",
      " arc 1: [1, 1, 2, 2]\n",
      " arc 2: [2, 2, 3, 3]\n",
      " arc 3: [1, 1, 3, 3]\n",
      "\n",
      "Total merged arcs: 3\n"
     ]
    }
   ],
   "source": [
    "M = snappy.Link('3_1')\n",
    "pd = M.PD_code() \n",
    "kinfo = analyze_pd_sage(pd)\n",
    "\n",
    "print(\"Original arcs (pre-merge):\", kinfo['original_arcs'])\n",
    "print(\"\\nMerged groups (gID -> original arcs):\")\n",
    "for gid, members in kinfo['merged']['groups'].items():\n",
    "    print(\" g{}: {}\".format(gid, members))\n",
    "print(\"\\nCrossings (merged tuples + signs):\")\n",
    "for ci,info in sorted(kinfo['crossings'].items()):\n",
    "    print(\" {}: pd_merged={}, sign={}, det(original)={:.6g}\".format(ci, info['pd_tuple_merged'], info['sign_label'], info['det_original']))\n",
    "print(\"\\nArc -> crossings (merged labels):\")\n",
    "for a,xs in sorted(kinfo['arc_to_crossings'].items()):\n",
    "    print(\" arc {}: {}\".format(a, xs))\n",
    "print(\"\\nTotal merged arcs:\", len(kinfo['arcs']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e3ef8c8-ebc6-423e-94ff-80b46267789d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arcs: [0, 1, 2, 3, 4, 5]\n",
      "Crossings summary:\n",
      " 1: pd=(5, 2, 0, 3), arcs=[0, 2, 3, 5], sign=+, det=0.865054\n",
      " 2: pd=(3, 0, 4, 1), arcs=[0, 1, 3, 4], sign=+, det=0.86651\n",
      " 3: pd=(1, 4, 2, 5), arcs=[1, 2, 4, 5], sign=+, det=0.86651\n",
      "Arc -> crossings: {0: [1, 2], 1: [2, 3], 2: [1, 3], 3: [1, 2], 4: [2, 3], 5: [1, 3]}\n"
     ]
    }
   ],
   "source": [
    "M = snappy.Link('3_1')\n",
    "pd = M.PD_code() \n",
    "kd = analyze_pd_sage(pd)\n",
    "\n",
    "print(\"Arcs:\", kd['arcs'])\n",
    "print(\"Crossings summary:\")\n",
    "for ci, info in kd['crossings'].items():\n",
    "    print(\" %d: pd=%s, arcs=%s, sign=%s, det=%.6g\" % (ci, str(info['pd_tuple']), str(info['arcs_involved']), info['sign_label'], info['det']))\n",
    "print(\"Arc -> crossings:\", kd['arc_to_crossings'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f522c2fa-308f-4feb-a18c-287f965363cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running analyze_pd_sage_debug ===\n",
      "Raw PD (as provided):\n",
      " 1: (5, 2, 0, 3)\n",
      " 2: (3, 0, 4, 1)\n",
      " 3: (1, 4, 2, 5)\n",
      "\n",
      "Arc occurrences (arc_label -> occurrences):\n",
      "  0: [(1, 'over_enter', 2), (2, 'under_exit', 1)]\n",
      "  1: [(2, 'over_exit', 3), (3, 'under_enter', 0)]\n",
      "  2: [(1, 'under_exit', 1), (3, 'over_enter', 2)]\n",
      "  3: [(1, 'over_exit', 3), (2, 'under_enter', 0)]\n",
      "  4: [(2, 'over_enter', 2), (3, 'under_exit', 1)]\n",
      "  5: [(1, 'under_enter', 0), (3, 'over_exit', 3)]\n",
      "\n",
      "Crossings (original, computed signs):\n",
      " 1: orig_pd=(5, 2, 0, 3), sign=+, det=0.865655\n",
      " 2: orig_pd=(3, 0, 4, 1), sign=+, det=0.86621\n",
      " 3: orig_pd=(1, 4, 2, 5), sign=+, det=0.86621\n",
      "\n",
      "Merge map (original -> group):\n",
      "  0 -> g 1\n",
      "  1 -> g 2\n",
      "  2 -> g 3\n",
      "  3 -> g 1\n",
      "  4 -> g 2\n",
      "  5 -> g 3\n",
      "\n",
      "Merged PD (tuples):\n",
      "  1 (3, 3, 1, 1)\n",
      "  2 (1, 1, 2, 2)\n",
      "  3 (2, 2, 3, 3)\n",
      "\n",
      "Final summary (merged tuples + signs):\n",
      " 1: pd_merged=(3, 3, 1, 1), sign=+, det(original)=0.865655\n",
      " 2: pd_merged=(1, 1, 2, 2), sign=+, det(original)=0.86621\n",
      " 3: pd_merged=(2, 2, 3, 3), sign=+, det(original)=0.86621\n",
      "\n",
      "Arc -> crossings (merged labels):\n",
      " arc 1 : [1, 1, 2, 2]\n",
      " arc 2 : [2, 2, 3, 3]\n",
      " arc 3 : [1, 1, 3, 3]\n",
      "\n",
      "Total merged arcs: 3\n"
     ]
    }
   ],
   "source": [
    "# Debuggable analyze_pd_sage for Sage (signs from original PD, then merge)\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---------------- geometry helpers ----------------\n",
    "def vector_sub(p, q): return (p[0]-q[0], p[1]-q[1])\n",
    "def det2(u, v): return u[0]*v[1] - u[1]*v[0]\n",
    "def normalize(v):\n",
    "    n = math.hypot(v[0], v[1])\n",
    "    return (v[0]/n, v[1]/n) if n else (0.0, 0.0)\n",
    "def safe_tangent(p_from, p_to):\n",
    "    v = vector_sub(p_to, p_from)\n",
    "    if abs(v[0]) < 1e-12 and abs(v[1]) < 1e-12:\n",
    "        v = (1e-6, 0.0)\n",
    "    return normalize(v)\n",
    "\n",
    "# ---------------- place arc points ----------------\n",
    "def place_arc_points(arcs, radius=1.0):\n",
    "    \"\"\"Place each original arc label at a distinct point on a circle (deterministic jitter).\"\"\"\n",
    "    arcs_sorted = list(sorted(arcs))\n",
    "    m = max(1, len(arcs_sorted))\n",
    "    pts = {}\n",
    "    for i, a in enumerate(arcs_sorted):\n",
    "        th = 2 * math.pi * i / m\n",
    "        j = ((int(a) * 37) % 1000 - 500) / 100000.0\n",
    "        th += j\n",
    "        pts[a] = (radius * math.cos(th), radius * math.sin(th))\n",
    "    return pts\n",
    "\n",
    "# ---------------- compute signs from original PD ----------------\n",
    "def compute_signs_from_pd(pd, debug=False):\n",
    "    \"\"\"\n",
    "    Input: pd list of 4-tuples (a,b,c,d) using standard PD convention\n",
    "           (under: a->b, over: c->d).\n",
    "    Returns (crossings, arc_set, occurrences) where crossings is dict(index->info).\n",
    "    \"\"\"\n",
    "    # Make sure pd is a list of 4-tuples\n",
    "    pd = [tuple(t) for t in pd]\n",
    "    # collect original arcs and occurrences\n",
    "    occurrences = defaultdict(list)   # arc_label -> list of (crossing_index, role, position_in_tuple)\n",
    "    for i, tup in enumerate(pd, start=1):\n",
    "        if len(tup) != 4:\n",
    "            raise ValueError(f\"PD entry {i} is not length 4: {tup}\")\n",
    "        a,b,c,d = tup\n",
    "        occurrences[a].append((i, 'under_enter', 0))\n",
    "        occurrences[b].append((i, 'under_exit', 1))\n",
    "        occurrences[c].append((i, 'over_enter', 2))\n",
    "        occurrences[d].append((i, 'over_exit', 3))\n",
    "    arc_set = sorted(occurrences.keys())\n",
    "\n",
    "    if debug:\n",
    "        print(\"Raw PD (as provided):\")\n",
    "        for i,tup in enumerate(pd, start=1):\n",
    "            print(f\" {i}: {tup}\")\n",
    "        print(\"\\nArc occurrences (arc_label -> occurrences):\")\n",
    "        for arc in arc_set:\n",
    "            print(f\"  {arc}: {occurrences[arc]}\")\n",
    "\n",
    "    # Basic sanity check: each arc should appear exactly twice (for a single component)\n",
    "    bad = [arc for arc, occ in occurrences.items() if len(occ) != 2]\n",
    "    if bad and debug:\n",
    "        print(\"\\nWarning: some arcs do not occur exactly twice (this may indicate a non-standard PD or multi-component link):\", bad)\n",
    "\n",
    "    # place points using original arc labels\n",
    "    pts = place_arc_points(arc_set)\n",
    "    crossings = {}\n",
    "    tol = 1e-9\n",
    "    for i, (a,b,c,d) in enumerate(pd, start=1):\n",
    "        p_a, p_b, p_c, p_d = pts[a], pts[b], pts[c], pts[d]\n",
    "        t_under = safe_tangent(p_b, p_a)\n",
    "        t_over = safe_tangent(p_d, p_c)\n",
    "        det = det2(t_under, t_over)\n",
    "        sign = None\n",
    "        if det > tol:\n",
    "            sign = 1\n",
    "        elif det < -tol:\n",
    "            sign = -1\n",
    "        else:\n",
    "            # small deterministic perturb attempt\n",
    "            sign = 0\n",
    "            for trial in range(6):\n",
    "                pts_trial = dict(pts)\n",
    "                for label in (a,b,c,d):\n",
    "                    x,y = pts_trial[label]\n",
    "                    eps = (0.00025 + 0.00005*trial) * (((int(label)*31) % 7) - 3)\n",
    "                    th = math.atan2(y,x) + eps\n",
    "                    r = math.hypot(x,y)\n",
    "                    pts_trial[label] = (r * math.cos(th), r * math.sin(th))\n",
    "                t_under = safe_tangent(pts_trial[b], pts_trial[a])\n",
    "                t_over = safe_tangent(pts_trial[d], pts_trial[c])\n",
    "                det_trial = det2(t_under, t_over)\n",
    "                if abs(det_trial) > tol:\n",
    "                    det = det_trial\n",
    "                    sign = 1 if det > 0 else -1\n",
    "                    pts = pts_trial\n",
    "                    break\n",
    "            if sign == 0:\n",
    "                # final fallback\n",
    "                sign = 1\n",
    "        crossings[i] = {\n",
    "            'pd_tuple_original': (a,b,c,d),\n",
    "            'under_original': (a,b),\n",
    "            'over_original': (c,d),\n",
    "            'det_original': float(det),\n",
    "            'sign': int(sign),\n",
    "            'sign_label': '+' if sign==1 else '-'\n",
    "        }\n",
    "    return crossings, arc_set, occurrences\n",
    "\n",
    "# ---------------- merge arcs by over-arc connectivity ----------------\n",
    "def merge_arcs_from_pd(pd, debug=False):\n",
    "    arc_set = set()\n",
    "    for a,b,c,d in pd:\n",
    "        arc_set.update([a,b,c,d])\n",
    "    parent = {x:x for x in arc_set}\n",
    "    def find(x):\n",
    "        while parent[x] != x:\n",
    "            parent[x] = parent[parent[x]]\n",
    "            x = parent[x]\n",
    "        return x\n",
    "    def union(x,y):\n",
    "        rx, ry = find(x), find(y)\n",
    "        if rx==ry:\n",
    "            return\n",
    "        # deterministically choose smaller rep\n",
    "        if rx < ry: parent[ry] = rx\n",
    "        else: parent[rx] = ry\n",
    "    # union over arcs only\n",
    "    for a,b,c,d in pd:\n",
    "        union(c,d)\n",
    "    groups = {}\n",
    "    for x in sorted(arc_set):\n",
    "        r = find(x)\n",
    "        groups.setdefault(r, []).append(x)\n",
    "    # produce stable gid assignment in sorted order of representative\n",
    "    reps = sorted(groups.keys())\n",
    "    rep_to_gid = {rep: (i+1) for i,rep in enumerate(reps)}\n",
    "    gid_to_members = {rep_to_gid[r]: groups[r] for r in reps}\n",
    "    label_to_group = {label: rep_to_gid[find(label)] for label in arc_set}\n",
    "    pd_merged = [(label_to_group[a], label_to_group[b], label_to_group[c], label_to_group[d]) for a,b,c,d in pd]\n",
    "    if debug:\n",
    "        print(\"\\nMerge map (original -> group):\")\n",
    "        for lab in sorted(label_to_group):\n",
    "            print(\" \", lab, \"-> g\", label_to_group[lab])\n",
    "        print(\"\\nMerged PD (tuples):\")\n",
    "        for i,t in enumerate(pd_merged, start=1):\n",
    "            print(\" \", i, t)\n",
    "    return {'groups': gid_to_members, 'label_to_group': label_to_group, 'pd_merged': pd_merged, 'reduced_arcs': sorted(gid_to_members.keys())}\n",
    "\n",
    "# ---------------- high-level analyzer (signs first, merge second) ----------------\n",
    "def analyze_pd_sage_debug(pd, debug=True):\n",
    "    \"\"\"\n",
    "    Computes signs on original PD, then merges arcs for naming.\n",
    "    Returns kinfo dict and prints debug info if debug=True.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(\"=== Running analyze_pd_sage_debug ===\")\n",
    "    crossings_raw, original_arcs, occurrences = compute_signs_from_pd(pd, debug=debug)\n",
    "    if debug:\n",
    "        print(\"\\nCrossings (original, computed signs):\")\n",
    "        for i,info in crossings_raw.items():\n",
    "            print(f\" {i}: orig_pd={info['pd_tuple_original']}, sign={info['sign_label']}, det={info['det_original']:.6g}\")\n",
    "\n",
    "    merged = merge_arcs_from_pd(pd, debug=debug)\n",
    "    pd_merged = merged['pd_merged']\n",
    "    merged_arcs = merged['reduced_arcs']\n",
    "\n",
    "    # Build final crossing records attaching merged tuple but keep original sign\n",
    "    crossings_final = {}\n",
    "    arc_to_crossings = {g:[] for g in merged_arcs}\n",
    "    for i, info in crossings_raw.items():\n",
    "        a,b,c,d = info['pd_tuple_original']\n",
    "        a_m, b_m, c_m, d_m = merged['label_to_group'][a], merged['label_to_group'][b], merged['label_to_group'][c], merged['label_to_group'][d]\n",
    "        crossings_final[i] = {\n",
    "            'pd_tuple_original': (a,b,c,d),\n",
    "            'pd_tuple_merged': (a_m,b_m,c_m,d_m),\n",
    "            'det_original': info['det_original'],\n",
    "            'sign': info['sign'],\n",
    "            'sign_label': info['sign_label']\n",
    "        }\n",
    "        for lab in (a_m,b_m,c_m,d_m):\n",
    "            arc_to_crossings.setdefault(lab, []).append(i)\n",
    "\n",
    "    kinfo = {\n",
    "        'original_arcs': original_arcs,\n",
    "        'merged': merged,\n",
    "        'arcs': merged_arcs,\n",
    "        'crossings': crossings_final,\n",
    "        'arc_to_crossings': arc_to_crossings,\n",
    "        'occurrences': occurrences\n",
    "    }\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\nFinal summary (merged tuples + signs):\")\n",
    "        for ci,inf in sorted(kinfo['crossings'].items()):\n",
    "            print(f\" {ci}: pd_merged={inf['pd_tuple_merged']}, sign={inf['sign_label']}, det(original)={inf['det_original']:.6g}\")\n",
    "        print(\"\\nArc -> crossings (merged labels):\")\n",
    "        for a,xs in sorted(kinfo['arc_to_crossings'].items()):\n",
    "            print(\" arc\", a, \":\", xs)\n",
    "        print(\"\\nTotal merged arcs:\", len(kinfo['arcs']))\n",
    "    return kinfo\n",
    "\n",
    "# ---------------- Example you can replace pd_example with your PD ----------------\n",
    "# Example PD for a trefoil-like input (you can paste your PD here)\n",
    "pd_example = [(5,2,0,3), (3,0,4,1), (1,4,2,5)]\n",
    "kinfo = analyze_pd_sage_debug(pd_example, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ae8e51f-ea5c-4cd4-8adc-0a4b98b4ed29",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 248)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mFile \u001b[39m\u001b[32m<string>:248\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mF, G, subres = analyze_wirtinger_and_reduce(kinfo)\u001b[39m\n                                                      ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Wirtinger relations -> group + substitution reducer\n",
    "# ===========================\n",
    "from sage.all import FreeGroup\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "\n",
    "# ---------- helper: convert merged arcs to generator names ----------\n",
    "def mk_gen_names(merged_arcs: List[int]) -> Dict[int,str]:\n",
    "    # map merged arc id -> generator symbol 'g1','g2',...\n",
    "    mapping = {}\n",
    "    for i, a in enumerate(sorted(merged_arcs), start=1):\n",
    "        mapping[a] = f\"g{i}\"\n",
    "    return mapping\n",
    "\n",
    "# ---------- create group presentation in Sage ----------\n",
    "def wirtinger_group_from_knotinfo(knot_info: Dict) -> Tuple:\n",
    "    \"\"\"\n",
    "    Input: knot_info returned from analyze_pd_sage (with merged arcs)\n",
    "    Returns: (F, gens_map, rels_words, G) where\n",
    "      - F is the FreeGroup\n",
    "      - gens_map maps merged-arc-id -> generator (Sage group generator object)\n",
    "      - rels_words is a list of Sage words (relations)\n",
    "      - G is the quotient group F / rels (presentation)\n",
    "    \"\"\"\n",
    "    merged = knot_info['merged']\n",
    "    merged_arcs = knot_info['arcs']  # list of merged arc ids\n",
    "    gens_map_names = mk_gen_names(merged_arcs)  # e.g. {1:'g1', ...}\n",
    "\n",
    "    # Build FreeGroup with those names\n",
    "    names = [gens_map_names[a] for a in sorted(merged_arcs)]\n",
    "    F = FreeGroup(names)\n",
    "    gens = F.gens()\n",
    "    # map names -> generator objects\n",
    "    name_to_gen = {str(g): g for g in gens}   # keys 'g1','g2',...\n",
    "    # map merged arc id -> generator object\n",
    "    arcid_to_gen = {a: name_to_gen[gens_map_names[a]] for a in merged_arcs}\n",
    "\n",
    "    # build relations according to signs\n",
    "    rels = []\n",
    "    for ci, info in sorted(knot_info['crossings'].items()):\n",
    "        # merged pd tuple\n",
    "        a_m, b_m, c_m, d_m = info['pd_tuple_merged']\n",
    "        sign = info['sign']\n",
    "        g_a = arcid_to_gen[a_m]\n",
    "        g_b = arcid_to_gen[b_m]\n",
    "        g_c = arcid_to_gen[c_m]\n",
    "        # positive crossing: g_b = g_c^-1 * g_a * g_c  -> relation r = g_c^-1*g_a*g_c * g_b^-1\n",
    "        if sign == 1:\n",
    "            rel_word = g_c**-1 * g_a * g_c * g_b**-1\n",
    "        else:\n",
    "            # negative crossing: g_a = g_c^-1 * g_b * g_c  -> r = g_c^-1*g_b*g_c * g_a^-1\n",
    "            rel_word = g_c**-1 * g_b * g_c * g_a**-1\n",
    "        rels.append(rel_word)\n",
    "\n",
    "    # quotient\n",
    "    G = F.quotient(rels)\n",
    "    return F, arcid_to_gen, rels, G\n",
    "\n",
    "# ---------- small symbolic substitution reducer ----------\n",
    "# We'll represent group words as lists of signed tokens like ['g1', 'g2^-1', ...]\n",
    "def word_to_tokens(word_str: str) -> List[str]:\n",
    "    \"\"\"Convert a string like 'g1^-1*g2*g3^-1' -> ['g1^-1','g2','g3^-1']\"\"\"\n",
    "    # normalize separators\n",
    "    tokens = [t for t in re.split(r'\\s*\\*\\s*', word_str) if t]\n",
    "    return tokens\n",
    "\n",
    "def invert_token(tok: str) -> str:\n",
    "    if tok.endswith('^-1'):\n",
    "        return tok[:-3]\n",
    "    else:\n",
    "        return tok + '^-1'\n",
    "\n",
    "def multiply_tokens(a: List[str], b: List[str]) -> List[str]:\n",
    "    # concatenate and reduce adjacent inverse pairs\n",
    "    res = a.copy()\n",
    "    for t in b:\n",
    "        if res and invert_token(t) == res[-1]:\n",
    "            res.pop()\n",
    "        else:\n",
    "            res.append(t)\n",
    "    return res\n",
    "\n",
    "def tokens_to_str(tokens: List[str]) -> str:\n",
    "    return '*'.join(tokens) if tokens else '1'\n",
    "\n",
    "def tokens_from_sage_word(sage_word) -> List[str]:\n",
    "    # sage_word.str() gives something like 'g1^-1*g2' — parse that\n",
    "    s = str(sage_word)\n",
    "    if s == '1':\n",
    "        return []\n",
    "    return word_to_tokens(s.replace('**-1','^-1').replace('**1',''))\n",
    "\n",
    "def substitute_once(eq_map: Dict[str,List[str]], tokens: List[str]) -> Tuple[List[str], bool]:\n",
    "    \"\"\"\n",
    "    If any token in `tokens` equals a key in eq_map, replace it by eq_map[token] tokens.\n",
    "    Return (new_tokens, changed)\n",
    "    Note: eq_map keys should be bare tokens like 'g1' or 'g1^-1'.\n",
    "    \"\"\"\n",
    "    changed = False\n",
    "    res = []\n",
    "    for t in tokens:\n",
    "        # check direct match\n",
    "        if t in eq_map:\n",
    "            changed = True\n",
    "            sub = eq_map[t]  # list of tokens to substitute in\n",
    "            # multiply res by sub (with cancellation)\n",
    "            res = multiply_tokens(res, sub)\n",
    "        else:\n",
    "            # also handle inverse substitution if inverse key exists\n",
    "            if t.endswith('^-1'):\n",
    "                base = t[:-3]\n",
    "                if base in eq_map:\n",
    "                    changed = True\n",
    "                    # substitute base then invert the substituted word\n",
    "                    sub = eq_map[base]\n",
    "                    # invert sub: reverse order and invert each token\n",
    "                    sub_inv = [invert_token(tok) for tok in reversed(sub)]\n",
    "                    res = multiply_tokens(res, sub_inv)\n",
    "                    continue\n",
    "            # otherwise keep token\n",
    "            res = multiply_tokens(res, [t])\n",
    "    return res, changed\n",
    "\n",
    "def reduce_relations_by_substitution(knot_info: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Attempt to reduce generators by substitution.\n",
    "    Returns a dict with:\n",
    "      - 'subs' : substitution mapping generator token -> token list\n",
    "      - 'reduced_relations' : list of remaining relations (as token lists)\n",
    "      - 'generators' : original generator names\n",
    "    \"\"\"\n",
    "    # build initial relations as tokens\n",
    "    merged_arcs = sorted(knot_info['arcs'])\n",
    "    gen_names = [f\"g{idx}\" for idx in range(1, len(merged_arcs)+1)]\n",
    "    # map merged arc ids (sorted) to generator names g1..gk\n",
    "    arcid_to_gname = {a: f\"g{i}\" for i,a in enumerate(merged_arcs, start=1)}\n",
    "    # inverse map if needed\n",
    "    # Build raw relations of the form lhs == rhs as tokens\n",
    "    eqs = []  # list of (lhs_token, rhs_tokens)\n",
    "    for ci, info in sorted(knot_info['crossings'].items()):\n",
    "        a_m, b_m, c_m, d_m = info['pd_tuple_merged']\n",
    "        sign = info['sign']\n",
    "        ga = arcid_to_gname[a_m]; gb = arcid_to_gname[b_m]; gc = arcid_to_gname[c_m]\n",
    "        if sign == 1:\n",
    "            # gb = gc^-1 * ga * gc\n",
    "            rhs = [gc + '^-1', ga, gc]\n",
    "            lhs = gb\n",
    "        else:\n",
    "            # ga = gc^-1 * gb * gc\n",
    "            rhs = [gc + '^-1', gb, gc]\n",
    "            lhs = ga\n",
    "        eqs.append((lhs, rhs))\n",
    "\n",
    "    # eq_map: token -> tokenlist for substitution (we keep substitutions like 'g2' -> ['g1','g3^-1'])\n",
    "    eq_map = {}\n",
    "    changed = True\n",
    "    iteration = 0\n",
    "    # Greedy substitution: if a relation has form single-lhs = rhs where rhs does not contain lhs,\n",
    "    # record it in eq_map and substitute into all other relations.\n",
    "    while changed and iteration < 100:\n",
    "        changed = False\n",
    "        iteration += 1\n",
    "        # find a substitution candidate\n",
    "        candidate = None\n",
    "        for lhs, rhs in eqs:\n",
    "            # if rhs is not trivial and lhs does not appear inside rhs (no self recursion), choose it\n",
    "            if lhs not in rhs:\n",
    "                # prefer RHS with fewer tokens (simpler substitution) — this is heuristic\n",
    "                candidate = (lhs, rhs)\n",
    "                break\n",
    "        if candidate:\n",
    "            lhs, rhs = candidate\n",
    "            # add to eq_map but first fully substitute existing eq_map into rhs\n",
    "            # (expand rhs tokens using existing eq_map recursively)\n",
    "            tokens = rhs\n",
    "            # apply existing substitutions until fixed\n",
    "            subchanged = True\n",
    "            while subchanged:\n",
    "                subchanged = False\n",
    "                new_tokens = []\n",
    "                for t in tokens:\n",
    "                    if t in eq_map:\n",
    "                        subchanged = True\n",
    "                        new_tokens = multiply_tokens(new_tokens, eq_map[t])\n",
    "                    elif t.endswith('^-1') and t[:-3] in eq_map:\n",
    "                        subchanged = True\n",
    "                        sub = eq_map[t[:-3]]\n",
    "                        sub_inv = [invert_token(tok) for tok in reversed(sub)]\n",
    "                        new_tokens = multiply_tokens(new_tokens, sub_inv)\n",
    "                    else:\n",
    "                        new_tokens = multiply_tokens(new_tokens, [t])\n",
    "                tokens = new_tokens\n",
    "            eq_map[lhs] = tokens\n",
    "            # remove any equations where lhs is on left (they're now recorded)\n",
    "            eqs = [(L,R) for (L,R) in eqs if L != lhs]\n",
    "            # substitute this new mapping into all rhs of remaining eqs\n",
    "            new_eqs = []\n",
    "            for L, R in eqs:\n",
    "                R_tokens = []\n",
    "                for t in R:\n",
    "                    if t in eq_map:\n",
    "                        R_tokens = multiply_tokens(R_tokens, eq_map[t])\n",
    "                    elif t.endswith('^-1') and t[:-3] in eq_map:\n",
    "                        sub = eq_map[t[:-3]]\n",
    "                        sub_inv = [invert_token(tok) for tok in reversed(sub)]\n",
    "                        R_tokens = multiply_tokens(R_tokens, sub_inv)\n",
    "                    else:\n",
    "                        R_tokens = multiply_tokens(R_tokens, [t])\n",
    "                # if substitution created L inside RHS (self-recursive), keep equation as is (no mapping)\n",
    "                new_eqs.append((L, R_tokens))\n",
    "            eqs = new_eqs\n",
    "            changed = True\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # After substitution loop, build remaining relation strings\n",
    "    reduced_relations = [(L, R) for L, R in eqs]\n",
    "    return {\n",
    "        'gens': gen_names,\n",
    "        'arcid_to_gname': {a: f\"g{i}\" for i,a in enumerate(sorted(knot_info['arcs']), start=1)},\n",
    "        'subs': eq_map,\n",
    "        'reduced_relations': reduced_relations\n",
    "    }\n",
    "\n",
    "# ----------------- Useful wrapper that does both Sage group and substitution -----------------\n",
    "def analyze_wirtinger_and_reduce(knot_info):\n",
    "    F, arcid_to_gen, rels, G = wirtinger_group_from_knotinfo(knot_info)\n",
    "    print(\"Sage free group gens:\", [str(g) for g in F.gens()])\n",
    "    print(\"Number of relations:\", len(rels))\n",
    "    print(\"Presentation (raw relations):\")\n",
    "    for r in rels:\n",
    "        print(\" \", r)\n",
    "    # Try substitutions\n",
    "    subres = reduce_relations_by_substitution(knot_info)\n",
    "    print(\"\\nSubstitution map (generator -> token list):\")\n",
    "    for k, v in subres['subs'].items():\n",
    "        print(\" \", k, \"->\", tokens_to_str(v))\n",
    "    print(\"\\nRemaining relations after greedy substitution:\")\n",
    "    for L, R in subres['reduced_relations']:\n",
    "        print(\" \", L, \"=\", tokens_to_str(R))\n",
    "    return F, G, subres\n",
    "\n",
    "# ===========================\n",
    "# Example usage (use your knot_info from analyze_pd_sage)\n",
    "# ===========================\n",
    "# If you already have kinfo from analyze_pd_sage:\n",
    "# F, G, subres = analyze_wirtinger_and_reduce(kinfo)\n",
    "\n",
    "# If you don't, you can create one quickly for the trefoil example:\n",
    "# Example format for knot_info expected by the function:\n",
    "#   knot_info = {\n",
    "#       'arcs': [1,2,3],\n",
    "#       'merged': {...},\n",
    "#       'crossings': {\n",
    "#            1: {'pd_tuple_merged': (3,3,1,1), 'sign': 1, ...},\n",
    "#            2: { ... },\n",
    "#            ...\n",
    "#       }\n",
    "#   }\n",
    "#\n",
    "# Replace the `kinfo_example` below with your actual kinfo (or call analyze_pd_sage above).\n",
    "#\n",
    "# Example (trefoil-like) to test the routine:\n",
    "kinfo_example = {\n",
    "    'arcs': [1,2,3],\n",
    "    'merged': {'groups': {1:[0,3], 2:[1,4], 3:[2,5]}, 'label_to_group': {}},\n",
    "    'crossings': {\n",
    "        1: {'pd_tuple_merged': (3,3,1,1), 'sign': 1},\n",
    "        2: {'pd_tuple_merged': (1,1,2,2), 'sign': 1},\n",
    "        3: {'pd_tuple_merged': (2,2,3,3), 'sign': 1}\n",
    "    }\n",
    "}\n",
    "# Uncomment to test:\n",
    "# F, G, subres = analyze_wirtinger_and_reduce(kinfo_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48ad777-9022-44c2-84eb-3277172e2b98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 10.7",
   "language": "sage",
   "name": "sagemath-10.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
